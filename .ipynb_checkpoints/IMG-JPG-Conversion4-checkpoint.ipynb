{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "rootDirectory = os.getcwd()\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_parent_urls(root_url):\n",
    "    url = root_url\n",
    "    r = requests.get(url)\n",
    "    urls = list()\n",
    "    soup = BeautifulSoup(r.content,'html.parser') \n",
    "    for link in soup.find_all('a'):\n",
    "        urls.append(url + link.get('href'))\n",
    "    return urls[5:]\n",
    "\n",
    "def generate_url(year, DOY):\n",
    "    url = 'https://pdsimage2.wr.usgs.gov/archive/mess-e_v_h-mdis-2-edr-rawdata-v1.0/MSGRMDS_1001/DATA/{}_{}/'.format(str(year), str(DOY))\n",
    "    return url\n",
    "\n",
    "def fetch_all_img_links_from_url(url):\n",
    "    \n",
    "    # create response object and parse with BeautifulSoup\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content,'html.parser') \n",
    "\n",
    "    # find all links on web-page \n",
    "    links = soup.findAll('a') \n",
    "\n",
    "    # filter the link sending with .IMG \n",
    "    IMG_Links = [url + link['href'] for link in links if link['href'].endswith('IMG')]\n",
    "    return(IMG_Links)\n",
    "\n",
    "def create_directory_for_img_files(year, DOY):\n",
    "    path = os.path.join(rootDirectory, '{}_{}'.format(str(year), str(DOY)))\n",
    "    try:  \n",
    "        os.mkdir(path)  \n",
    "    except OSError as error:  \n",
    "        print(error)\n",
    "        \n",
    "    return path\n",
    "    \n",
    "def change_directory(path):\n",
    "    os.chdir(path)\n",
    "    cwd = path\n",
    "    \n",
    "def download_img_files(inDirectory, IMG_Links):\n",
    "    change_directory(inDirectory)\n",
    "    \n",
    "    for link in IMG_Links:\n",
    "        r = requests.get(link, stream = True)\n",
    "        file_name = link.split('/')[-1]\n",
    "        \n",
    "        #download started\n",
    "        print(\"Downloading {}\".format(file_name))\n",
    "        with open(file_name, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size = 1024*1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(\"Downloaded {}\".format(file_name))\n",
    "    print (\"All files downloaded!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"img_urls.txt\", \"r\") as f:\n",
    "    img_urls = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Harshit's MacBook Pro External 500GB HDD/CRISMIS\r\n"
     ]
    }
   ],
   "source": [
    "# print(len(img_urls))\n",
    "\n",
    "!pwd\n",
    "\n",
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ', printEnd = \"\\r\"):\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end =\"\\r\")\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EW0211364448C.IMG', 'EN0234705245M.IMG', 'EW0225229183I.IMG', 'EW0244488547G.IMG', 'EN0141935782M.IMG', 'EW0050494951A.IMG', 'EW0215591586G.IMG', 'EN0235172720M.IMG', 'EW0224932544J.IMG', 'EN0212196154M.IMG']\n",
      "122440\n"
     ]
    }
   ],
   "source": [
    "fileList = list()\n",
    "os.chdir(rootDirectory)\n",
    "for root, dirs, files in os.walk(\"IMG_Data\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".IMG\"):\n",
    "             fileList.append(file)\n",
    "print(fileList[:10])\n",
    "print(len(fileList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Harshit's MacBook Pro External 500GB HDD/CRISMIS/IMG_Data\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import subprocess\n",
    "os.chdir(\"IMG_Data\")\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# import cv2\n",
    "def convert_img_jpg(filename, errorCount):\n",
    "    rawData  = open(filename, 'rb').read()\n",
    "#     print(filename)\n",
    "    # File size in bytes\n",
    "    fs       = len(rawData)\n",
    "    bitDepth = int(re.search(\"SAMPLE_BITS\\s+=\\s+(\\d+)\",str(rawData)).group(1))\n",
    "    bytespp  = int(bitDepth/8)\n",
    "    height   = int(re.search(\"LINES\\s+=\\s+(\\d+)\",str(rawData)).group(1))\n",
    "    width    = int(re.search(\"LINE_SAMPLES\\s+=\\s+(\\d+)\",str(rawData)).group(1))\n",
    "#     print(bitDepth,height,width)\n",
    "\n",
    "    # Offset from start of file to image data - assumes image at tail end of file\n",
    "    offset = fs - (width*height*bytespp)\n",
    "\n",
    "    # Check bitDepth\n",
    "    try:\n",
    "        if bitDepth == 8:\n",
    "            na = np.frombuffer(rawData, offset=offset, dtype=np.uint8).reshape(height,width)\n",
    "            Image.fromarray(na).save(\"/Volumes/Harshit's MacBook Pro External 500GB HDD/CRISMIS/dataset-v3/{}.jpg\".format(filename))\n",
    "        elif bitDepth == 16:\n",
    "            dt = np.dtype(np.uint16)\n",
    "            dt = dt.newbyteorder('>')\n",
    "            na = np.frombuffer(rawData, offset=offset, dtype=dt).reshape(height,width).astype(np.uint8)\n",
    "            Image.fromarray(na).save(\"/Volumes/Harshit's MacBook Pro External 500GB HDD/CRISMIS/dataset-v3/{}.jpg\".format(filename))\n",
    "        else:\n",
    "#             print('ERROR: Unexpected bit depth')\n",
    "            pass\n",
    "            \n",
    "    except:\n",
    "        errorCount += 1\n",
    "#         print(\"error converting{}\".format(filename))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |--------------------------------------------------| 0.6% Complete\r"
     ]
    }
   ],
   "source": [
    "printProgressBar(0, len(fileList), prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "\n",
    "for i, file in enumerate(fileList):\n",
    "\terrorCount = 0\n",
    "\tfileName = str(file)\n",
    "\tconvert_img_jpg(fileName, errorCount)\n",
    "    # \tprint(dest)\n",
    "    # \tprint(fileName)\n",
    "    # \tprint(\"converted\", str(file))\n",
    "\tprintProgressBar(i + 1, len(fileList), prefix = 'Progress:', suffix = 'Complete', length = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from PIL import Image\n",
    "# os.chdir(\"IMG_Data\")\n",
    "for file in fileList:\n",
    "    rawData = open(file, 'rb').read()\n",
    "    size = re.search(\"(LINES              = \\d\\d\\d\\d)|(LINES              = \\d\\d\\d)\", str(rawData))\n",
    "    pixelDepth = re.search(\"(SAMPLE_BITS        = \\d\\d)|(SAMPLE_BITS        = \\d)\", str(rawData))\n",
    "    size = (str(size)[-6:-2])\n",
    "    pixelDepth = (str(pixelDepth)[-4:-2])\n",
    "    print(int(size), end='  ')\n",
    "    print(int(pixelDepth), end='    ')\n",
    "    imgSize = (int(size), int(size))\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        img = Image.frombytes('L', imgSize, rawData)\n",
    "        img.save(str(file)+'.jpg')\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "seconds = time.time()\n",
    "print(\"Seconds since epoch =\", seconds)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year, DOY = input(\"Enter Year and DOY: \").split(\" \")\n",
    "url = generate_url(year, DOY)\n",
    "IMG_Links = fetch_all_img_links_from_url(url)\n",
    "path = create_directory_for_img_files(year, DOY)\n",
    "change_directory(path)\n",
    "\n",
    "download_img_files(inDirectory=path, IMG_Links=IMG_Links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "img_urls = list()\n",
    "urls = return_parent_urls(\"https://pdsimage2.wr.usgs.gov/archive/mess-e_v_h-mdis-2-edr-rawdata-v1.0/MSGRMDS_1001/DATA/\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "\n",
    "for url in urls:\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content,'html.parser') \n",
    "\n",
    "    # find all links on web-page \n",
    "    links = soup.findAll('a') \n",
    "\n",
    "    # filter the link sending with .IMG \n",
    "    IMG_Links = [url + link['href'] for link in links if link['href'].endswith('IMG')]\n",
    "    img_urls.extend(IMG_Links)\n",
    "    \n",
    "    counter+=1\n",
    "    print(counter, end='\\t\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
